<!DOCTYPE html><html><head><title>ScalaPB: SparkSQL</title><meta charset="utf-8" /><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Nadav Samet" /><meta name="description" content="Protocol buffer compiler for Scala" /><meta name="og:image" content="/img/poster.png" /><meta name="image" property="og:image" content="/img/poster.png" /><meta name="og:title" content="ScalaPB: SparkSQL" /><meta name="title" property="og:title" content="ScalaPB: SparkSQL" /><meta name="og:site_name" content="ScalaPB" /><meta name="og:url" content="" /><meta name="og:type" content="website" /><meta name="og:description" content="Protocol buffer compiler for Scala" /><link rel="icon" type="image/png" href="/img/favicon.png" /><meta name="twitter:title" content="ScalaPB: SparkSQL" /><meta name="twitter:image" content="/img/poster.png" /><meta name="twitter:description" content="Protocol buffer compiler for Scala" /><meta name="twitter:card" content="summary_large_image" /><link rel="icon" type="image/png" sizes="16x16" href="/img/favicon16x16.png" /><link rel="icon" type="image/png" sizes="24x24" href="/img/favicon24x24.png" /><link rel="icon" type="image/png" sizes="32x32" href="/img/favicon32x32.png" /><link rel="icon" type="image/png" sizes="48x48" href="/img/favicon48x48.png" /><link rel="icon" type="image/png" sizes="57x57" href="/img/favicon57x57.png" /><link rel="icon" type="image/png" sizes="60x60" href="/img/favicon60x60.png" /><link rel="icon" type="image/png" sizes="64x64" href="/img/favicon64x64.png" /><link rel="icon" type="image/png" sizes="70x70" href="/img/favicon70x70.png" /><link rel="icon" type="image/png" sizes="72x72" href="/img/favicon72x72.png" /><link rel="icon" type="image/png" sizes="76x76" href="/img/favicon76x76.png" /><link rel="icon" type="image/png" sizes="96x96" href="/img/favicon96x96.png" /><link rel="icon" type="image/png" sizes="114x114" href="/img/favicon114x114.png" /><link rel="icon" type="image/png" sizes="120x120" href="/img/favicon120x120.png" /><link rel="icon" type="image/png" sizes="128x128" href="/img/favicon128x128.png" /><link rel="icon" type="image/png" sizes="144x144" href="/img/favicon144x144.png" /><link rel="icon" type="image/png" sizes="150x150" href="/img/favicon150x150.png" /><link rel="icon" type="image/png" sizes="152x152" href="/img/favicon152x152.png" /><link rel="icon" type="image/png" sizes="196x196" href="/img/favicon196x196.png" /><link rel="icon" type="image/png" sizes="310x310" href="/img/favicon310x310.png" /><link rel="icon" type="image/png" sizes="310x150" href="/img/favicon310x150.png" /><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" /><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" /><link rel="stylesheet" href="/highlight/styles/atom-one-light.css" /><link rel="stylesheet" href="/css/pattern-style.css" /><link rel="stylesheet" href="/css/custom.css" /><script async="async">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-346180-20' , 'auto');
ga('send', 'pageview');
      </script></head><body class="docs"><div id="wrapper"><div id="sidebar-wrapper"><ul id="sidebar" class="sidebar-nav"><li class="sidebar-brand"><a href="/" class="brand"><div class="brand-wrapper"><span>ScalaPB</span></div></a></li> <li><a href="/" class="">About</a></li> <li><a href="/sbt-settings.html" class="">SBT Settings</a></li> <li><a href="/generated-code.html" class="">Generated code</a></li> <li><a href="/customizations.html" class="">Customizations</a></li> <li><a href="/sealed-oneofs.html" class="">Sealed oneofs</a></li> <li><a href="/user_defined_options.html" class="">User-defined options</a></li> <li><a href="/grpc.html" class="">gRPC</a></li> <li><a href="/scala.js.html" class="">Scala.js</a></li> <li><a href="/sparksql.html" class=" active ">SparkSQL</a></li> <li><a href="/json.html" class="">JSON</a></li> <li><a href="/scalapbc.html" class="">ScalaPBC</a></li> <li><a href="/third-party-protos.html" class="">Using third-party protos</a></li> <li><a href="/common-protos.html" class="">Common protos</a></li> <li><a href="/api/scalapb/latest/" class="">API Docs</a></li> <li><a href="/faq.html" class="">FAQ</a></li> <li><a href="/migrating.html" class="">Migrating</a></li> <li><a href="/contact.html" class="">Contact us</a></li></ul></div><div id="page-content-wrapper"><div class="nav"><div class="container-fluid"><div class="row"><div class="col-lg-12"><div class="action-menu pull-left clearfix"><a href="#menu-toggle" id="menu-toggle"><i class="fa fa-bars" aria-hidden="true"></i></a></div><ul class="pull-right"><li id="gh-eyes-item" class="hidden-xs"><a href="https://github.com/scalapb/ScalaPB" target="_blank" rel="noopener noreferrer"><i class="fa fa-eye"></i><span>Watchers<span id="eyes" class="label label-default">--</span></span></a></li><li id="gh-stars-item" class="hidden-xs"><a href="https://github.com/scalapb/ScalaPB" target="_blank" rel="noopener noreferrer"><i class="fa fa-star-o"></i><span>Stars<span id="stars" class="label label-default">--</span></span></a></li><li><a href="#" onclick="shareSiteTwitter('ScalaPB Protocol buffer compiler for Scala');"><i class="fa fa-twitter"></i></a></li><li><a href="#" onclick="shareSiteFacebook('ScalaPB Protocol buffer compiler for Scala');"><i class="fa fa-facebook"></i></a></li><li><a href="#" onclick="shareSiteGoogle();"><i class="fa fa-google-plus"></i></a></li></ul></div></div></div></div><div id="content" data-github-owner="scalapb" data-github-repo="ScalaPB"><div class="content-wrapper"><section><h1 id="scalapb-with-sparksql">ScalaPB with SparkSQL</h1>

<h2 id="introduction">Introduction</h2>

<p>By default, Spark uses reflection to derive schemas and encoders from case
classes. This doesn’t work well when there are messages that contain types that
Spark does not understand such as enums, <code class="language-plaintext highlighter-rouge">ByteString</code>s and <code class="language-plaintext highlighter-rouge">oneof</code>s. To get around this, sparksql-scalapb provides its own <code class="language-plaintext highlighter-rouge">Encoder</code>s for protocol buffers.</p>

<p>However, it turns out there is another obstacle. Spark does not provide any mechanism to compose user-provided encoders with its own reflection-derived Encoders. Therefore, merely providing an <code class="language-plaintext highlighter-rouge">Encoder</code> for protocol buffers is insufficient to derive an encoder for regular case-classes that contain a protobuf as a field. To solve this problem, ScalaPB uses <a href="https://github.com/typelevel/frameless">frameless</a> which relies on implicit search to derive encoders. This approach enables combining ScalaPB’s encoders with frameless encoders that takes care for all non-protobuf types.</p>

<h2 id="setting-up-your-project">Setting up your project</h2>

<p>Make sure that you are using ScalaPB 0.9.0 or later.</p>

<p>We are going to use sbt-assembly to deploy a fat JAR containing ScalaPB, and
your compiled protos.  Make sure in project/plugins.sbt you have a line
that adds sbt-assembly:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">addSbtPlugin</span><span class="o">(</span><span class="s">"com.eed3si9n"</span> <span class="o">%</span> <span class="s">"sbt-assembly"</span> <span class="o">%</span> <span class="s">"0.14.10"</span><span class="o">)</span>
</code></pre></div></div>

<p>In <code class="language-plaintext highlighter-rouge">build.sbt</code> add a dependency on <code class="language-plaintext highlighter-rouge">sparksql-scalapb</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">libraryDepenencies</span> <span class="o">+=</span> <span class="s">"com.thesamet.scalapb"</span> <span class="o">%%</span> <span class="s">"sparksql-scalapb"</span> <span class="o">%</span> <span class="s">"0.9.0"</span>
</code></pre></div></div>

<p>Spark ships with an old version of Google’s Protocol Buffers runtime that is not compatible with
the current version. Therefore, we need to shade our copy of the Protocol Buffer runtime.
Add this to your build.sbt:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">assemblyShadeRules</span> <span class="n">in</span> <span class="n">assembly</span> <span class="o">:=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="nv">ShadeRule</span><span class="o">.</span><span class="py">rename</span><span class="o">(</span><span class="s">"com.google.protobuf.**"</span> <span class="o">-&gt;</span> <span class="s">"shadeproto.@1"</span><span class="o">).</span><span class="py">inAll</span>
<span class="o">)</span>
</code></pre></div></div>

<p>See <a href="https://github.com/thesamet/sparksql-scalapb-test/blob/master/build.sbt">complete example of build.sbt</a>.</p>

<h2 id="using-sparksql-scalapb">Using sparksql-scalapb</h2>

<p>We assume you have a <code class="language-plaintext highlighter-rouge">SparkSession</code> assigned to the variable <code class="language-plaintext highlighter-rouge">spark</code>. In a standalone Scala program, this can be created with:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">val</span> <span class="nv">spark</span><span class="k">:</span> <span class="kt">SparkSession</span> <span class="o">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="py">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="py">appName</span><span class="o">(</span><span class="s">"ScalaPB Demo"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">master</span><span class="o">(</span><span class="s">"local[2]"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">getOrCreate</span><span class="o">()</span>
<span class="c1">// spark: SparkSession = org.apache.spark.sql.SparkSession@6f57e9a2</span>
</code></pre></div></div>

<p><em>IMPORTANT</em>: Ensure you do not import <code class="language-plaintext highlighter-rouge">spark.implicits._</code> to avoid ambiguity between ScalaPB provided encoders and Spark’s default encoders. You may want to import <code class="language-plaintext highlighter-rouge">StringToColumn</code> to convert <code class="language-plaintext highlighter-rouge">$"col name"</code> into a <code class="language-plaintext highlighter-rouge">Column</code>. Add an import <code class="language-plaintext highlighter-rouge">scaslapb.spark.Implicits</code> to add ScalaPB’s encoders for protocol buffers into the implicit search scope:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.sql.</span><span class="o">{</span><span class="nc">Dataset</span><span class="o">,</span> <span class="nc">DataFrame</span><span class="o">,</span> <span class="n">functions</span> <span class="k">=&gt;</span> <span class="n">F</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">spark.implicits.StringToColumn</span>
<span class="k">import</span> <span class="nn">scalapb.spark.ProtoSQL</span>

<span class="k">import</span> <span class="nn">scalapb.spark.Implicits._</span>
</code></pre></div></div>

<p>The code snippets below use the <a href="https://github.com/scalapb/ScalaPB/blob/master/docs/src/main/protobuf/person.proto"><code class="language-plaintext highlighter-rouge">Person</code> message</a>.</p>

<p>We start by creating some test data:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">scalapb.docs.person.Person</span>
<span class="k">import</span> <span class="nn">scalapb.docs.person.Person.</span><span class="o">{</span><span class="nc">Address</span><span class="o">,</span> <span class="nc">AddressType</span><span class="o">}</span>

<span class="k">val</span> <span class="nv">testData</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
   <span class="nc">Person</span><span class="o">(</span><span class="n">name</span><span class="o">=</span><span class="s">"John"</span><span class="o">,</span> <span class="n">age</span><span class="k">=</span><span class="mi">32</span><span class="o">,</span> <span class="n">addresses</span><span class="k">=</span><span class="nc">Vector</span><span class="o">(</span>
     <span class="nc">Address</span><span class="o">(</span><span class="n">addressType</span><span class="k">=</span><span class="nv">AddressType</span><span class="o">.</span><span class="py">HOME</span><span class="o">,</span> <span class="n">street</span><span class="o">=</span><span class="s">"Market"</span><span class="o">,</span> <span class="n">city</span><span class="o">=</span><span class="s">"SF"</span><span class="o">))</span>
   <span class="o">),</span>
   <span class="nc">Person</span><span class="o">(</span><span class="n">name</span><span class="o">=</span><span class="s">"Mike"</span><span class="o">,</span> <span class="n">age</span><span class="k">=</span><span class="mi">29</span><span class="o">,</span> <span class="n">addresses</span><span class="k">=</span><span class="nc">Vector</span><span class="o">(</span>
     <span class="nc">Address</span><span class="o">(</span><span class="n">addressType</span><span class="k">=</span><span class="nv">AddressType</span><span class="o">.</span><span class="py">WORK</span><span class="o">,</span> <span class="n">street</span><span class="o">=</span><span class="s">"Castro"</span><span class="o">,</span> <span class="n">city</span><span class="o">=</span><span class="s">"MV"</span><span class="o">),</span>
     <span class="nc">Address</span><span class="o">(</span><span class="n">addressType</span><span class="k">=</span><span class="nv">AddressType</span><span class="o">.</span><span class="py">HOME</span><span class="o">,</span> <span class="n">street</span><span class="o">=</span><span class="s">"Church"</span><span class="o">,</span> <span class="n">city</span><span class="o">=</span><span class="s">"MV"</span><span class="o">))</span>
   <span class="o">),</span>
   <span class="nc">Person</span><span class="o">(</span><span class="n">name</span><span class="o">=</span><span class="s">"Bart"</span><span class="o">,</span> <span class="n">age</span><span class="k">=</span><span class="mi">27</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div></div>

<p>We can create a <code class="language-plaintext highlighter-rouge">DataFrame</code> from the test data:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">ProtoSQL</span><span class="o">.</span><span class="py">createDataFrame</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="n">testData</span><span class="o">)</span>
<span class="c1">// df: DataFrame = [name: string, age: int ... 1 more field]</span>
<span class="nv">df</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">//  |-- name: string (nullable = true)</span>
<span class="c1">//  |-- age: integer (nullable = true)</span>
<span class="c1">//  |-- addresses: array (nullable = false)</span>
<span class="c1">//  |    |-- element: struct (containsNull = false)</span>
<span class="c1">//  |    |    |-- address_type: string (nullable = true)</span>
<span class="c1">//  |    |    |-- street: string (nullable = true)</span>
<span class="c1">//  |    |    |-- city: string (nullable = true)</span>
<span class="c1">// </span>
<span class="nv">df</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +----+---+--------------------+</span>
<span class="c1">// |name|age|           addresses|</span>
<span class="c1">// +----+---+--------------------+</span>
<span class="c1">// |John| 32|[[HOME, Market, SF]]|</span>
<span class="c1">// |Mike| 29|[[WORK, Castro, M...|</span>
<span class="c1">// |Bart| 27|                  []|</span>
<span class="c1">// +----+---+--------------------+</span>
<span class="c1">//</span>
</code></pre></div></div>

<p>and then process it as any other Dataframe in Spark:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">df</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="n">$</span><span class="s">"name"</span><span class="o">,</span> <span class="nv">F</span><span class="o">.</span><span class="py">size</span><span class="o">(</span><span class="n">$</span><span class="s">"addresses"</span><span class="o">).</span><span class="py">alias</span><span class="o">(</span><span class="s">"address_count"</span><span class="o">)).</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +----+-------------+</span>
<span class="c1">// |name|address_count|</span>
<span class="c1">// +----+-------------+</span>
<span class="c1">// |John|            1|</span>
<span class="c1">// |Mike|            2|</span>
<span class="c1">// |Bart|            0|</span>
<span class="c1">// +----+-------------+</span>
<span class="c1">// </span>

<span class="k">val</span> <span class="nv">nameAndAddress</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="n">$</span><span class="s">"name"</span><span class="o">,</span> <span class="n">$</span><span class="s">"addresses"</span><span class="o">.</span><span class="py">getItem</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">alias</span><span class="o">(</span><span class="s">"firstAddress"</span><span class="o">))</span>
<span class="c1">// nameAndAddress: DataFrame = [name: string, firstAddress: struct&lt;address_type: string, street: string ... 1 more field&gt;]</span>

<span class="nv">nameAndAddress</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +----+------------------+</span>
<span class="c1">// |name|      firstAddress|</span>
<span class="c1">// +----+------------------+</span>
<span class="c1">// |John|[HOME, Market, SF]|</span>
<span class="c1">// |Mike|[WORK, Castro, MV]|</span>
<span class="c1">// |Bart|              null|</span>
<span class="c1">// +----+------------------+</span>
<span class="c1">//</span>
</code></pre></div></div>

<p>Using the datasets API it is possible to bring the data back to ScalaPB case classes:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">nameAndAddress</span><span class="o">.</span><span class="py">as</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Option</span><span class="o">[</span><span class="kt">Address</span><span class="o">])].</span><span class="py">collect</span><span class="o">().</span><span class="py">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
<span class="c1">// (John,Some(Address(HOME,Market,SF,UnknownFieldSet(Map()))))</span>
<span class="c1">// (Mike,Some(Address(WORK,Castro,MV,UnknownFieldSet(Map()))))</span>
<span class="c1">// (Bart,None)</span>
</code></pre></div></div>

<p>You can create a Dataset directly using Spark APIs:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">spark</span><span class="o">.</span><span class="py">createDataset</span><span class="o">(</span><span class="n">testData</span><span class="o">)</span>
<span class="c1">// res5: Dataset[Person] = [name: string, age: int ... 1 more field]</span>
</code></pre></div></div>

<h2 id="from-binary-to-protos-and-back">From Binary to protos and back</h2>

<p>In some situations, you may need to deal with datasets that contain serialized protocol buffers. This can be handled by mapping the datasets through ScalaPB’s <code class="language-plaintext highlighter-rouge">parseFrom</code> and <code class="language-plaintext highlighter-rouge">toByteArray</code> functions.</p>

<p>Let’s start by preparing a dataset with test binary data by mapping our <code class="language-plaintext highlighter-rouge">testData</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">binaryDS</span><span class="k">:</span> <span class="kt">Dataset</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Byte</span><span class="o">]]</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">createDataset</span><span class="o">(</span><span class="nv">testData</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">toByteArray</span><span class="o">))</span>
<span class="c1">// binaryDS: Dataset[Array[Byte]] = [_1: binary]</span>

<span class="nv">binaryDS</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |                  _1|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |[0A 04 4A 6F 68 6...|</span>
<span class="c1">// |[0A 04 4D 69 6B 6...|</span>
<span class="c1">// |[0A 04 42 61 72 7...|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">//</span>
</code></pre></div></div>

<p>To turn this dataset into a <code class="language-plaintext highlighter-rouge">Dataset[Person]</code>, we map it through <code class="language-plaintext highlighter-rouge">parseFrom</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">protosDS</span><span class="k">:</span> <span class="kt">Dataset</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="nv">binaryDS</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="nv">Person</span><span class="o">.</span><span class="py">parseFrom</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
<span class="c1">// protosDS: Dataset[Person] = [name: string, age: int ... 1 more field]</span>
</code></pre></div></div>

<p>to turn a dataset of protos into <code class="language-plaintext highlighter-rouge">Dataset[Array[Byte]]</code>:</p>
<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">protosBinary</span><span class="k">:</span> <span class="kt">Dataset</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Byte</span><span class="o">]]</span> <span class="k">=</span> <span class="nv">protosDS</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">toByteArray</span><span class="o">)</span>
<span class="c1">// protosBinary: Dataset[Array[Byte]] = [_1: binary]</span>
</code></pre></div></div>

<h2 id="on-enums">On enums</h2>

<p>In SparkSQL-ScalaPB, enums are represented as strings. Unrecognized enum values are represented as strings containing the numeric value.</p>

<h2 id="dataframes-and-datasets-from-rdds">Dataframes and Datasets from RDDs</h2>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.rdd.RDD</span>

<span class="k">val</span> <span class="nv">protoRDD</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">testData</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">protoDF</span><span class="k">:</span> <span class="kt">DataFrame</span> <span class="o">=</span> <span class="nv">ProtoSQL</span><span class="o">.</span><span class="py">protoToDataFrame</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="n">protoRDD</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">protoDS</span><span class="k">:</span> <span class="kt">Dataset</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">createDataset</span><span class="o">(</span><span class="n">protoRDD</span><span class="o">)</span>
</code></pre></div></div>

<h2 id="udfs">UDFs</h2>

<p>If you need to write a UDF that returns a message, it would not pick up our encoder and you may get a runtime failure.  To work around this, sparksql-scalapb provides <code class="language-plaintext highlighter-rouge">ProtoSQL.udf</code> to create UDFs. For example, if you need to parse a binary column into a proto:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">binaryDF</span> <span class="k">=</span> <span class="nv">protosBinary</span><span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"value"</span><span class="o">)</span>
<span class="c1">// binaryDF: DataFrame = [value: binary]</span>

<span class="k">val</span> <span class="nv">parsePersons</span> <span class="k">=</span> <span class="nv">ProtoSQL</span><span class="o">.</span><span class="py">udf</span> <span class="o">{</span> <span class="n">bytes</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Byte</span><span class="o">]</span> <span class="k">=&gt;</span> <span class="nv">Person</span><span class="o">.</span><span class="py">parseFrom</span><span class="o">(</span><span class="n">bytes</span><span class="o">)</span> <span class="o">}</span>
<span class="c1">// parsePersons: org.apache.spark.sql.Column =&gt; org.apache.spark.sql.Column = scalapb.spark.Udfs$$Lambda$14015/0x0000000804dfd040@5b45dfc3</span>

<span class="nv">binaryDF</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"person"</span><span class="o">,</span> <span class="nf">parsePersons</span><span class="o">(</span><span class="n">$</span><span class="s">"value"</span><span class="o">))</span>
<span class="c1">// res7: DataFrame = [value: binary, person: struct&lt;name: string, age: int ... 1 more field&gt;]</span>
</code></pre></div></div>

<h2 id="primitive-wrappers">Primitive wrappers</h2>

<p>In ProtoSQL 0.9.x and 0.10.x, primitive wrappers are represented in Spark as structs
witha single field named <code class="language-plaintext highlighter-rouge">value</code>. A better representation in Spark would be a
nullable field of the primitive type. The better representation will be the
default in 0.11.x. To enable this representation today, replace the usages of
<code class="language-plaintext highlighter-rouge">scalapb.spark.ProtoSQL</code> with <code class="language-plaintext highlighter-rouge">scalapb.spark.ProtoSQL.withPrimitiveWrappers</code>.
Instead of importing <code class="language-plaintext highlighter-rouge">scalapb.spark.Implicits._</code>, import
<code class="language-plaintext highlighter-rouge">scalapb.spark.ProtoSQL.implicits._</code></p>

<p>See example in <a href="https://github.com/scalapb/sparksql-scalapb/blob/80f3162b69313d57f95d3dcbfee865809873567a/sparksql-scalapb/src/test/scala/WrappersSpec.scala#L42-L59">WrappersSpec</a>.</p>

<h2 id="datasets-and-none-is-not-a-term">Datasets and <code class="language-plaintext highlighter-rouge">&lt;none&gt; is not a term</code></h2>

<p>You will see this error if for some reason Spark’s <code class="language-plaintext highlighter-rouge">Encoder</code>s are being picked up
instead of the ones provided by sparksql-scalapb. Please ensure you are not importing <code class="language-plaintext highlighter-rouge">spark.implicits._</code>. See instructions above for imports.</p>

<h2 id="example">Example</h2>

<p>Check out a <a href="https://github.com/thesamet/sparksql-scalapb-test">complete example</a> here.</p>
</section></div></div></div></div><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script><script src="/highlight/highlight.pack.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/protobuf.min.js"></script><script>
// For all code blocks, copy the language from the containing div
// to the inner code tag (where hljs expects it to be)
const langPrefix = 'language-';
document.querySelectorAll(`div[class^='${langPrefix}']`).forEach(function(div) {
  div.classList.forEach(function(cssClass) {
    if (cssClass.startsWith(langPrefix)) {
      const lang = cssClass.substring(langPrefix.length);
      div.querySelectorAll('pre code').forEach(function(code) {
        code.classList.add(lang);
      });
    }
  });
});

hljs.configure({languages:['scala','java','bash','protobuf']});
hljs.initHighlightingOnLoad();
      </script><script>console.info('\x57\x65\x62\x73\x69\x74\x65\x20\x62\x75\x69\x6c\x74\x20\x77\x69\x74\x68\x3a\x0a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x5f\x5f\x20\x20\x20\x20\x5f\x5f\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x5f\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x5f\x20\x5f\x5f\x0a\x20\x20\x20\x5f\x5f\x5f\x5f\x5f\x2f\x20\x2f\x5f\x20\x20\x2f\x20\x2f\x5f\x20\x20\x20\x20\x20\x20\x5f\x5f\x5f\x5f\x20\x5f\x5f\x5f\x20\x20\x28\x5f\x29\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x5f\x20\x20\x5f\x5f\x5f\x5f\x5f\x28\x5f\x29\x20\x2f\x5f\x5f\x5f\x5f\x20\x20\x5f\x5f\x5f\x5f\x5f\x0a\x20\x20\x2f\x20\x5f\x5f\x5f\x2f\x20\x5f\x5f\x20\x5c\x2f\x20\x5f\x5f\x2f\x5f\x5f\x5f\x5f\x5f\x2f\x20\x5f\x5f\x20\x60\x5f\x5f\x20\x5c\x2f\x20\x2f\x20\x5f\x5f\x5f\x2f\x20\x5f\x5f\x5f\x2f\x20\x5f\x5f\x20\x5c\x2f\x20\x5f\x5f\x5f\x2f\x20\x2f\x20\x5f\x5f\x2f\x20\x5f\x20\x5c\x2f\x20\x5f\x5f\x5f\x2f\x0a\x20\x28\x5f\x5f\x20\x20\x29\x20\x2f\x5f\x2f\x20\x2f\x20\x2f\x5f\x2f\x5f\x5f\x5f\x5f\x5f\x2f\x20\x2f\x20\x2f\x20\x2f\x20\x2f\x20\x2f\x20\x2f\x20\x2f\x5f\x5f\x2f\x20\x2f\x20\x20\x2f\x20\x2f\x5f\x2f\x20\x28\x5f\x5f\x20\x20\x29\x20\x2f\x20\x2f\x5f\x2f\x20\x20\x5f\x5f\x28\x5f\x5f\x20\x20\x29\x0a\x2f\x5f\x5f\x5f\x5f\x2f\x5f\x2e\x5f\x5f\x5f\x2f\x5c\x5f\x5f\x2f\x20\x20\x20\x20\x20\x2f\x5f\x2f\x20\x2f\x5f\x2f\x20\x2f\x5f\x2f\x5f\x2f\x5c\x5f\x5f\x5f\x2f\x5f\x2f\x20\x20\x20\x5c\x5f\x5f\x5f\x5f\x2f\x5f\x5f\x5f\x5f\x2f\x5f\x2f\x5c\x5f\x5f\x2f\x5c\x5f\x5f\x5f\x2f\x5f\x5f\x5f\x5f\x2f\x0a\x0a\x68\x74\x74\x70\x73\x3a\x2f\x2f\x34\x37\x64\x65\x67\x2e\x67\x69\x74\x68\x75\x62\x2e\x69\x6f\x2f\x73\x62\x74\x2d\x6d\x69\x63\x72\x6f\x73\x69\x74\x65\x73')</script><script>((window.gitter = {}).chat = {}).options = {
room: 'ScalaPB/community'};</script><script src="https://sidecar.gitter.im/dist/sidecar.v1.js"></script><script src="/js/main.js"></script></body></html>